;p_id;pid_type;year;journal;publisher;title;book_title;abstract;oabstract;authors;editors;language;meshterms;keywords;coi;grant;doi
1;35194431;Article;2021;Iranian journal of pharmaceutical research : IJPR;;"Pharmacogenomics Implementation and Hurdles to Overcome; In the Context of a Developing Country.";;Having multiple dimensions, uncertainties and several stakeholders, the costly pharmacogenomics (PGx) is associated with dynamic implementation complexities. Identification of these challenges is critical to harness its full potential, especially in developing countries with fragile healthcare systems and scarce resources. This is the first study aimed to identify most salient challenges related to PGx implementation, with respect to the experiences of early-adopters  and local experts' prospects, in the context of a developing country in the Middle East. To perform a comprehensive reconnaissance on PGx adoption challenges a scoping literature review was conducted based on national drug policy components: efficacy/safety, access, affordability and rational use of medicine (RUM). Strategic option development and analysis workshop method with cognitive mapping as the technique was used to evaluate challenges in the context of Iran.  The cognitive maps were face-validated and analyzed via Decision Explorer XML. The findings indicated a complex network of issues relative to PGx adoption, categorized in national drug policy indicators. In the rational use of medicine category, ethics, education, bench -to- bedside strategies, guidelines, compliance, and health system issues were found. Clinical trial issues, test's utility, and biomarker validation were identified in the efficacy group. Affordability included pricing, reimbursement, and value assessment issues. Finally, access category included regulation, availability, and stakeholder management challenges. The current study identified the most significant challenges ahead of clinical implementation of PGx in a developing country. This  could be the basis of a policy-note development in future work, which may consolidate vital communication among stakeholders and accelerate the efficient implementation in developing new-comer countries.;;Ayati N, Afzali M, Hasanzad M, Kebriaeezadeh A, Rajabzadeh A, Nikfar S;;eng;;['Developing countries', 'Dynamic challenges', 'Iran', 'Pharmacogenomics', 'Policy note', 'Precision medicine', 'Regulatory framework'];The authors declare no conflict of interest.;;10.22037/ijpr.2021.114899.15091
2;35106138;Article;2021;F1000Research;;Improving the support for XML dynamic updates using a hybridization labeling scheme (ORD-GAP).;;Background : As the standard for the exchange of data over the World Wide Web, it is important to ensure that the eXtensible Markup Language (XML) database is capable of supporting not only efficient query processing but also capable of enduring frequent data update operations over the dynamic changes of Web content. Most of the existing XML annotation is based on a labeling scheme to identify each hierarchical position of the XML nodes. This computation is costly as any updates will cause the whole XML tree to be re-labelled. This impact can be observed on large datasets. Therefore, a robust labeling scheme that avoids re-labeling is crucial. Method: Here, we present ORD-GAP (named after Order Gap), a robust and persistent XML labeling scheme that supports dynamic updates. ORD-GAP assigns unique identifiers with gaps in-between XML nodes, which could easily identify the level, Parent-Child (P-C), Ancestor-Descendant (A-D) and sibling relationship. ORD-GAP adopts the OrdPath labeling scheme for any future insertion. Results: We demonstrate that ORD-GAP is robust enough for dynamic updates, and have implemented it in three use cases: (i) left-most, (ii) in-between and (iii) right-most insertion. Experimental evaluations on DBLP dataset demonstrated that ORD-GAP outperformed existing approaches such as ORDPath and ME Labeling concerning database storage size, data loading time and query retrieval. On average, ORD-GAP has the best storing and query retrieval time. Conclusion: The main contributions of this paper are: (i) A robust labeling scheme named ORD-GAP that assigns certain gap between each node to support future insertion, and (ii) An efficient mapping scheme, which built upon ORD-GAP labeling scheme to transform XML into RDB effectively.;;Haw SC, Amin A, Wong CO, Subramaniam S;;eng;;['XML databases', 'XML labeling scheme.', 'XML-RDB mapping', 'dynamic updates', 'mapping scheme'];No competing interests were disclosed.;;10.12688/f1000research.69108.1
3;35028636;Article;2022;Journal of mass spectrometry and advances in the clinical lab;;Listening to your mass spectrometer: An open-source toolkit to visualize mass spectrometer data.;;"Introduction: We have developed a set of tools built with open-source software that includes both a database and a visualization component to collect LC-MS/MS data and monitor quality control parameters. Description of tool: To display LC-MS/MS data we built a parsing tool using Python and standard libraries to parse the XML files after each clinical run. The tool parses the necessary information to store a database comprised of three distinct tables. Another component to this toolkit is an interactive data visualization tool that uses the data from the database. There are 5 different visualizations that present the data based on interchangeable parameters. Evaluation of tool: Using histogram visualization, we assessed how quality control parameters that feed our quality control algorithm, SMACK, which assists to improve the efficiency of data review  and results, performed against the collective data. Using the newly identified QC parameter values from the toolkit, we compared the output of the SMACK algorithm; the number of QC flags changed in that there was a 1.7% (31/1944 observations) increase in flags and a 7.1% (138/1944 observations) decrease in presumed false positive flags, increasing the overall performance of SMACK which helped staff focus their time on reviewing more concerning QC failures. Discussion: We have developed a customizable web-based dashboard for instrument performance monitoring for our opiate confirmation LC-MS/MS assay using data collected with each batch. The web-based platform allows users to monitor instrument performance and can encompass other instruments throughout the laboratory. This information can help the laboratory take proactive measures to maintain instruments, ultimately reducing the amount down time needed for maintenance.";;Pablo A, Hoofnagle AN, Mathias PC;;eng;;['Dashboard', 'Database', 'GB, Gigabyte', 'LC-MS/MS, Liquid chromatography tandem mass spectrometry', 'LLOQ, Lower limit of quantification', 'MB, Megabyte', 'Mass spectrometry', 'Python', 'QC, Quality control', 'Quality control', 'RRT, Relative retention time', 'Visualization'];The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.;;10.1016/j.jmsacl.2021.12.003
4;34972171;Article;2021;PloS one;;Medical data integration using HL7 standards for patient's early identification.;;"Integration between information systems is critical, especially in the healthcare domain, since interoperability requirements are related to patients' data confidentiality, safety, and satisfaction. The goal of this study is to propose a solution based on the integration between queue management solution (QMS) and the electronic medical records (EMR), using Health Level Seven (HL7) protocols and Extensible Markup Language (XML). The proposed solution facilitates the patient's self-check-in within a healthcare organization in UAE. The solution aims to help  in minimizing the waiting times within the outpatient department through early identification of patients who hold the Emirates national ID cards, i.e., whether an Emirati or expatriates. The integration components, solution design, and the custom-designed XML and HL7 messages were clarified in this paper. In addition, the study includes a simulation experiment through control and intervention weeks with 517 valid appointments. The experiment goal was to evaluate the patient's total journey and each related clinical stage by comparing the ""routine-based identification"" with the ""patient's self-check-in"" processes in case of booked appointments. As a key finding, the proposed solution is efficient and could reduce the ""patient's journey time"" by more than 14 minutes and ""time to identify"" patients by 10 minutes. There was also a significant drop in the waiting time to triage and the time to finish the triage process. In conclusion,  the proposed solution is considered innovative and can provide a positive added value for the patient's whole journey.";;AlQudah AA, Al-Emran M, Shaalan K;;eng;['*Appointments and Schedules', 'Computer Security', 'Confidentiality', '*Data Collection', 'Delivery of Health Care', '*Electronic Health Records', '*Health Level Seven', 'Humans', 'Medical Informatics/*methods', '*Outpatients', 'Patient Safety', 'Patient Satisfaction', 'Programming Languages', 'Risk Assessment', 'Software', '*Systems Integration', 'Triage', 'United Arab Emirates', 'Workflow'];;The authors have declared that no competing interests exist.;;10.1371/journal.pone.0262067
5;34916929;Article;2021;Frontiers in pharmacology;;Efficacy and Safety of Traditional Chinese Medicine Injections for Heart Failure  With Reduced Ejection Fraction: A Bayesian Network Meta-Analysis of Randomized Controlled Trials.;;"Background: Heart failure as an important issue in global public health, has brought a heavy economic burden. Traditional Chinese medicine injections (TCMIs)  have significant effects on heart failure with reduced ejection fraction (HFrEF). However, it is difficult for clinicians to identify the differences in clinical efficacy and safety of various TCMIs. The purpose of this study is to compare the efficacy and safety of various TCMIs for treating HFrEF by conducting a Bayesian  network meta-analysis (NMA) and to further provide references for clinical decision-making. Methods: The clinical randomized controlled trials of TCMIs for  treating HFrEF were searched in seven database from inception to August 3rd, 2021. The Cochrane collaboration's tool was used to assess the risk of bias. NMA  was performed in a Bayesian hierarchical framework. The surface under the cumulative ranking curve (SUCRA), the multi-dimensional efficacy analysis, the comparison-adjusted funnel plot, and the node-splitting analysis were conducted using R software. Results: A total of 107 eligible RCTs involving 9,073 HFrEF patients and 6 TCMIs were included. TCMIs include Huangqi injection (HQ) also called Astragalus injection, Shenfu injection (SF), Shengmai injection (SGM), Shenmai injection (SM), Xinmailong injection (XML), and Yiqifumai lyophilized injection (YQFM). The results of NMA and SUCRA showed that with conventional treatment (CT) as a common control, in terms of clinical efficacy, CT + XML was most effective in New York Heart Association cardiac functional classification efficiency, brain natriuretic peptide, and N-terminal pro-brain natriuretic peptide; the CT + SM was most effective in 6-min walking test, left ventricular end-diastolic diameter, left ventricular end-systolic diameter and cardiac output; the CT + YQFM was most effective in left ventricular ejection fraction; the CT + HQ was most effective in stroke volume; the CT + SF was most effective in Minnesota Living with Heart Failure Questionnaire. In terms of safety, there was no significant difference between CT + TCMIs and CT. Conclusion: This Bayesian network meta-analysis results show that the combination of qualified TCMIs and CT is more effective for HFrEF patients than CT alone, and CT + XML and CT + SM may be one of the potential optimal treatments. Also, the safety of these TCMIs needs to be further observed. However, due to some limitations, the conclusions need to be verified by more large-sample, double-blind, multi-center  RCTs.";;Lin S, Shi Q, Ge Z, Liu Y, Cao Y, Yang Y, Zhao Z, Bi Y, Hou Y, Wang S, Wang X, Mao J;;eng;;['bayesian model', 'heart failure', 'network meta-analysis', 'randomized controlled trial', 'traditional chinese medicine', 'traditional chinese medicine injection'];The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. The reviewer LH declared a shared affiliation with some of  the authors, SL,QS, ZG, YL, YC, YY, to the handling editor at time of review.;;10.3389/fphar.2021.659707
6;34890097;Article;2021;The FEBS journal;;EnzymeML-a data exchange format for biocatalysis and enzymology.;;EnzymeML is an XML-based data exchange format that supports the comprehensive documentation of enzymatic data by describing reaction conditions, time courses of substrate and product concentrations, the kinetic model, and the estimated kinetic constants. EnzymeML is based on the Systems Biology Markup Language, which was extended by implementing the STRENDA Guidelines. An EnzymeML document serves as a container to transfer data between experimental platforms, modeling tools, and databases. EnzymeML supports the scientific community by introducing a standardized data exchange format to make enzymatic data findable, accessible, interoperable, and reusable according to the FAIR data principles. An application programming interface in Python supports the integration of software tools for data acquisition, data analysis, and publication. The feasibility of a seamless data flow using EnzymeML is demonstrated by creating an EnzymeML document from a  structured spreadsheet or from a STRENDA DB database entry, by kinetic modeling using the modeling platform COPASI, and by uploading to the enzymatic reaction kinetics database SABIO-RK.;;Range J, Halupczok C, Lohmann J, Swainston N, Kettner C, Bergmann FT, Weidemann A, Wittig U, Schnell S, Pleiss J;;eng;;['FAIR data principles', 'Python', 'Systems Biology Markup Language', 'XML', 'biocatalysis', 'bioinformatics', 'data exchange', 'enzymology', 'research data management'];;EXC310/Deutsche Forschungsgemeinschaft;10.1111/febs.16318
7;34770684;Article;2021;Sensors (Basel, Switzerland);;Control and Diagnostics System Generator for Complex FPGA-Based Measurement Systems.;;FPGA-based data acquisition and processing systems play an important role in modern high-speed, multichannel measurement systems, especially in High-Energy and Plasma Physics. Such FPGA-based systems require an extended control and diagnostics part corresponding to the complexity of the controlled system. Managing the complex structure of registers while keeping the tight coupling between hardware and software is a tedious and potentially error-prone process. Various existing solutions aimed at helping that task do not perfectly match all  specific requirements of that application area. The paper presents a new solution based on the XML system description, facilitating the automated generation of the control system's HDL code and software components and enabling easy integration with the control software. The emphasis is put on reusability, ease of maintenance in the case of system modification, easy detection of mistakes, and the possibility of use in modern FPGAs. The presented system has been successfully used in data acquisition and preprocessing projects in high-energy physics experiments. It enables easy creation and modification of the control system definition and convenient access to the control and diagnostic blocks. The presented system is an open-source solution and may be adopted by the user for particular needs.;;Zabolotny WM, Guminski M, Kruszewski M, Muller WFJ;;eng;['*Computers', '*Software'];['FPGA', 'VHDL', 'Wishbone', 'control interface', 'system diagnostics', 'system management'];;;10.3390/s21217378
8;34760250;Article;2021;Food science & nutrition;;The effect of curculigo orchioides (Xianmao) on kidney energy metabolism and the  related mechanism in rats based on metabolomics.;;The Chinese materia medica Xianmao (XM) is widely used in Chinese clinics and the traditional Chinese medicine diets. Although XM is often used to study its kidney-yang effect, the research on its effect on kidney energy metabolism and its mechanism is still relatively lacking. In this study, rats were given different doses of XM water extract for 4 weeks. Biochemical method was used to detect the content of serum biochemical indexes of liver and kidney function and  blood lipid indicators, and HE staining method was used to observe the histopathological of liver and kidney in rats. The kidney Na(+)-K(+)-ATPase, Ca(2+)-Mg(2+)-ATPase, SDH (succinate dehydrogenase) enzyme activity, and the content of ATP in rats were measured. Metabolomics technology was used to analyze the potential biomarkers related to the effects of XM on kidney energy metabolism, and then, the metabolic pathways were analyzed. RT-PCR was used to detect the expression of Ampk, Sirt1, Ppar-alpha, and Pgc-1alpha mRNA in kidney of rats. The results showed, compared with the blank control group, there was no  significant effect on liver and kidney function in XMH, XMM, and XML groups. These significantly increased the kidney Na(+)-K(+)-ATPase, Ca(2+)-Mg(2+)-ATPase, SDH enzyme activity, and ATP content in XMH, XMM, and XML groups. Mitochondrial metabolic rate was inhibited in XMH group, but it was significantly increased in  XMM and XML groups. The number of mitochondria was increased in XMH, XMM, and XML groups. Overall, these effects may be mediated by TCA cycle metabolism, butanoate metabolism, propanoate metabolism, alanine, aspartate, and glutamate metabolism,  retinol metabolism, purine metabolism, pentose phosphate metabolism, aminoacyl-tRNA biosynthesis, valine, leucine, and isoleucine biosynthesis, and degradation metabolism pathways, as well as by increasing expression of upstream  genes Ampk, Sirt1, Ppar-alpha, and Pgc-1alpha mRNA.;;Chen L, Qu B, Wang H, Liu H, Guan Y, Zhou J, Zhang J;;eng;;['RT-PCR', 'Xianmao', 'kidney energy metabolism', 'metabolomics'];The authors declared no potential conflicts of interest with respect to the research, authorship, and publication of this article.;;10.1002/fsn3.2573
9;34734333;Article;2021;Protoplasma;;Construction of an N6-methyladenosine lncRNA- and immune cell infiltration-related prognostic model in colorectal cancer.;;"The present paper aims to shed light on the influence of N6-methyladenosine (m6A) long non-coding RNAs (lncRNAs) and immune cell infiltration on colorectal cancer  (CRC). We downloaded workflow-type data and xml-format clinical data on CRC from  The Cancer Genome Atlas project. The relationship between lncRNA and m6A was identified by using Perl and R software. Kyoto Encyclopedia of Genes and Genomes  enrichment analysis was performed. Lasso regression was utilized to construct a prognostic model. Survival analysis was used to explore the relationship between  clusters of m6A lncRNAs and clinical survival data. Differential analysis of the  tumor microenvironment and an immune correlation analysis were used to determine  immune cell infiltration levels in different clusters and their correlation with  clinical prognosis. The expression of lncRNA was tightly associated with m6A. The univariate Cox regression analysis showed that lncRNA was a risk factor for the prognosis. Differential expression analysis demonstrated that m6A lncRNAs were partially highly expressed in tumor tissue. m6A lncRNA-related prognostic model could predict the prognosis of CRC independently. ""ECM_RECEPTOR_INTERACTION"" was  the most significantly enriched gene set. PARP8 was overexpressed in tumor tissue and high-risk cluster. CD4 memory T cells, activated resting NK cells, and memory B cells were highly clustered in the high-risk cluster. All of the scores were higher in the low-risk group. m6A lncRNA is closely related to the occurrence and progression of CRC. The corresponding prognostic model can be utilized to evaluate the prognosis of CRC. m6A lncRNA and related immune cell infiltration in the tumor microenvironment can provide novel therapeutic targets for further research.";;Yu ZL, Zhu ZM;;eng;;['Bioinformatics analysis', 'Colorectal cancer', 'Immune cell infiltration', 'N6-Methyladenosine', 'lncRNAs'];;81860433/National Natural Science Foundation of China;10.1007/s00709-021-01718-x
10;34720253;Article;2021;Scientometrics;;"Software review: The JATSdecoder package-extract metadata, abstract and sectioned text from NISO-JATS coded XML documents; Insights to PubMed central's open access database.";;JATSdecoder is a general toolbox which facilitates text extraction and analytical tasks on NISO-JATS coded XML documents. Its function JATSdecoder() outputs metadata, the abstract, the sectioned text and reference list as easy selectable  elements. One of the biggest repositories for open access full texts covering biology and the medical and health sciences is PubMed Central (PMC), with more than 3.2 million files. This report provides an overview of the PMC document collection processed with JATSdecoder(). The development of extracted tags is displayed for the full corpus over time and in greater detail for some meta tags. Possibilities and limitations for text miners working with scientific literature  are outlined. The NISO-JATS-tags are used quite consistently nowadays and allow a reliable extraction of metadata and text elements. International collaborations are more present than ever. There are obvious errors in the date stamps of some documents. Only about half of all articles from 2020 contain at least one author  listed with an author identification code. Since many authors share the same name, the identification of person-related content is problematic, especially for authors with Asian names. JATSdecoder() reliably extracts key metadata and text elements from NISO-JATS coded XML files. When combined with the rich, publicly available content within PMCs database, new monitoring and text mining approaches can be carried out easily. Any selection of article subsets should be carefully performed with in- and exclusion criteria on several NISO-JATS tags, as both the  subject and keyword tags are used quite inconsistently.;;Boschen I;;eng;;['Meta-research', 'PubMed central', 'Software', 'Text extraction', 'Text mining'];Conflict of interestThe author declares no conflict of interest.;;10.1007/s11192-021-04162-z
11;34593888;Article;2021;Scientific reports;;Evaluation of JATSdecoder as an automated text extraction tool for statistical results in scientific reports.;;The extraction of statistical results in scientific reports is beneficial for checking studies on plausibility and reliability. The R package JATSdecoder supports the application of text mining approaches to scientific reports. Its function get.stats() extracts all reported statistical results from text and recomputes p values for most standard test results. The output can be reduced to  results with checkable or computable p values only. In this article, get.stats()'s ability to extract, recompute and check statistical results is compared to that of statcheck, which is an already established tool. A manually coded data set, containing the number of statistically significant results in 49  articles, serves as an initial indicator for get.stats()'s and statcheck's differing detection rates for statistical results. Further 13,531 PDF files by 10 mayor psychological journals, 18,744 XML documents by Frontiers of Psychology and 23,730 articles related to psychological research and published by PLoS One are scanned for statistical results with both algorithms. get.stats() almost replicates the manually extracted number of significant results in 49 PDF articles. get.stats() outperforms the statcheck functions in identifying statistical results in every included journal and input format. Furthermore, the  raw results extracted by get.stats() increase statcheck's detection rate. JATSdecoder's function get.stats() is a highly general and reliable tool to extract statistical results from text. It copes with a wide range of textual representations of statistical standard results and recomputes p values for two-  and one-sided tests. It facilitates manual and automated checks on consistency and completeness of the reported results within a manuscript.;;Boschen I;;eng;;;;;10.1038/s41598-021-98782-3
12;34579510;Article;2021;Turkish journal of medical sciences;;Could computed tomography histogram analysis add value to the diagnosis of cholesteatoma.;;"BACKGROUND/AIM: To investigate the potential role of computed tomography (CT) histogram analysis in differentiating cholesteatoma (CHS) and non-cholesteatoma (NCHS). MATERIALS AND METHODS: We evaluated 77 temporal bone CT images (from November 2016 to February 2020) that were obtained pre-operatively (mean age, 37.10+/-17.27 years in CHS; 36.72+/-16.08 years in NCHS group). Histogram analyses of the resulting XML files were conducted using the R Project 3.3.2 program. ROC analysis was used to find threshold values and the diagnostic efficiency of these values in differentiating CHS-NCHS was determined. RESULTS: The CT images of 41 CHS (53.25%) and 36 NHCS cases (46.75%) were evaluated. There was a statistically significant difference between the CHS and NCHS group in terms of the mean, maximum, and median values ( P=0.036, P=0.006, P=0.043). When  examining the ROC curve obtained from the mean of these parameters, area under the curve (AUC)=0.638, and when the threshold value is selected as 42.55, the mean value was determined to have a sensitivity of 86.50% and specificity of 56.10% in differentiating CHS-NCHS. CONCLUSION: In cases with magnetic resonance  imaging (MRI) contraindications, small-sized lesions may be difficult to detect and characterize due to a poor resolution; to reduce the rate of false positives/negatives in these situations, CT histogram analysis of previously taken images may provide the additional information.";;Yurttutan N, Bilal N, Kizildag B, Doganer A;;eng;;['Histogram analysis', 'cholesteatoma', 'computed tomography', 'temporal bone'];;;10.3906/sag-2105-12
13;34563332;Article;2021;Journal of electrocardiology;;Computerized automated algorithm-based analyses of digitized paper ECGs in Brugada syndrome.;;"BACKGROUND: Brugada syndrome is a rare inherited arrhythmic syndrome with a coved type 1 ST-segment elevation on ECG and an increased risk of sudden death. Many studies have evaluated risk stratification performance based on ECG-derived parameters. However, since historical Brugada patient cohorts included mostly paper ECGs, most studies have been based on manual ECG parameter measurements. We hypothesized that it would be possible to run automated algorithm-based analysis  of paper ECGs. We aimed: 1) to validate the digitization process for paper ECGs in Brugada patients; and 2) to quantify the acute class I antiarrhythmic drug effect on relevant ECG parameters in Brugada syndrome. METHODS: A total of 176 patients (30% female, 43 +/- 13 years old) with induced type 1 Brugada syndrome ECG were included in the study. All of the patients had paper ECGs before and during class I antiarrhythmic drug challenge. Twenty patients also had a digital  ECG, in whom printouts were used to validate the digitization process. Paper ECGs were scanned and then digitized using ECGScan software, version 3.4.0 (AMPS, LLC, New York, NY, USA) to obtain FDA HL7 XML format ECGs. Measurements were automatically performed using the Bravo (AMPS, LLC, New York, NY, USA) and Glasgow algorithms. RESULTS: ECG parameters obtained from digital and digitized ECGs were closely correlated (r = 0.96 +/- 0.07, R(2) = 0.93 +/- 0.12). Class I antiarrhythmic drugs significantly increased the global QRS duration (from 113 +/- 20 to 138 +/- 23, p < 0.0001). On lead V2, class I antiarrhythmic drugs increased ST-segment elevation (from 110 +/- 84 to 338 +/- 227 muV, p < 0.0001),  decreased the ST slope (from 14.9 +/- 23.3 to -27.4 +/- 28.5, p < 0.0001) and increased the TpTe interval (from 88 +/- 18 to 104 +/- 33, p < 0.0001). CONCLUSIONS: Automated algorithm-based measurements of depolarization and repolarization parameters from digitized paper ECGs are reliable and could quantify the acute effects of class 1 antiarrhythmic drug challenge in Brugada patients. Our results support using computerized automated algorithm-based analyses from digitized paper ECGs to establish risk stratification decision trees in Brugada syndrome.";;Extramiana F, Laporte PL, Vaglio M, Denjoy I, Maison-Blanche P, Badilini F, Leenhardt A;;eng;['Adult', 'Algorithms', 'Anti-Arrhythmia Agents/therapeutic use', '*Brugada Syndrome/diagnosis/drug therapy', 'Electrocardiography', 'Female', 'Humans', 'Male', 'Middle Aged', 'Software'];['*Brugada syndrome', '*Computerized analysis', '*Digitization', '*ECG'];;;10.1016/j.jelectrocard.2021.09.009
14;34545825;Article;2021;Studies in health technology and informatics;;ADT2FHIR - A Tool for Converting ADT/GEKID Oncology Data to HL7 FHIR Resources.;;Harmonized and interoperable data management is a core requirement for federated  infrastructures in clinical research. Institutions participating in such infrastructures often have to invest large degrees of time and resources in implementing necessary data integration processes to convert their local data to  the required target structure. If the data is already available in an alternative shared data structure, the transformation from source to the desired target structure can be implemented once and then be distributed to all participants to  reduce effort and harmonize results. The HL7(R) FHIR(R) standard is used as a basis for the shared data model of several medical consortia like DKTK and GBA. It is based on so-called resources which can be represented in XML. Oncological data in German university hospitals is commonly available in the ADT/GEKID format. From this common basis we conceptualized and implemented a transformation which accepts ADT/GEKID XML files and returns FHIR resources. We identified several problems with using the general ADT/GEKID structure in federated research infrastructures, as well as some possible pitfalls relating to the FHIR need for  resource ids and focus on semantic coding which differs from the approach in the  ADT/GEKID standard. To facilitate participation in federated infrastructures, we  propose the ADT2FHIR transformation tool for partners with oncological data in the ADT/GEKID format.;;Deppenwiese N, Delpy P, Lambarki M, Lablans M;;eng;['*Data Management', '*Electronic Health Records', 'Health Level Seven', 'Humans', 'Medical Oncology', 'Semantics'];['Data Management', 'FHIR', 'Health Information Interoperability', 'Medical Oncology', 'Open Source'];;;10.3233/SHTI210547
15;34504584;Article;2021;Experimental and therapeutic medicine;;Periplaneta americana extract ameliorates lipopolysaccharide-induced liver injury by improving mitochondrial dysfunction via the AMPK/PGC-1alpha signaling pathway.;;"Periplaneta americana (PA) extract acts clinically as a therapeutic treatment in  various diseases; it enhances liver function in mouse models and mitigates the pathological condition of liver fibrosis. The present study aimed to investigate  the role and potential mechanisms underlying the action of the PA extract, xinmailong (XML), in lipopolysaccharide (LPS)-induced liver injury. Following the treatment of AML12 cells with LPS, the content of cytochrome c in the cytoplasm and mitochondria, and the level of ATP synthesis were detected using corresponding kits. The relative mRNA expression levels of nuclear respiratory factor 1 and transcription factor A, mitochondrial were investigated using reverse transcription-quantitative (RT-q)PCR analysis. The MTT assay was performed to detect the viability of AML12 cells following treatment with XML, in the absence or presence of LPS. Western blot analysis was performed to determine  the expression levels of proteins in the AMP-activated protein kinase (AMPK)/proliferator-activated receptor gamma coactivator-1alpha (PGC-1alpha) pathway. Following treatment with compound C, an inhibitor of AMPK, the expression levels of inflammatory cytokines were determined using ELISA and RT-qPCR analysis. The levels of oxidative stress-related markers were detected using corresponding kits following treatment with compound C. In addition, TUNEL  staining was performed to detect the apoptosis of AML12 cells, and western blot analysis was performed to investigate the expression levels of apoptosis-related  proteins. Mitochondrial dysfunction was induced by LPS in AML12 cells. LPS stimulation significantly downregulated the expression of proteins in the AMPK/PGC-1alpha pathway, which was reversed following treatment with XML. In addition, inflammation, oxidative stress and mitochondrial dysfunction induced by LPS were alleviated by XML in AML12 cells. However, the addition of compound C and XML to LPS-induced AML12 cells resulted in the aggravation of cell injury. Collectively, the results of the present study indicated that XML suppressed mitochondrial dysfunction induced by LPS by activating AMPK/PGC-1alpha signaling. Thus, the results of the present study may contribute to further understanding of the underlying mechanism via which XML alleviates liver injury.";;Shi W, An L, Zhang J, Li J;;eng;;['AMP-activated protein kinase', 'inflammation', 'lipopolysaccharide', 'liver injury', 'mitochondrial dysfunction'];The authors declare that they have no competing interests.;;10.3892/etm.2021.10572
16;34497870;Article;2021;PeerJ. Computer science;;Efficient processing of complex XSD using Hive and Spark.;;The eXtensible Markup Language (XML) files are widely used by the industry due to their flexibility in representing numerous kinds of data. Multiple applications such as financial records, social networks, and mobile networks use complex XML schemas with nested types, contents, and/or extension bases on existing complex elements or large real-world files. A great number of these files are generated each day and this has influenced the development of Big Data tools for their parsing and reporting, such as Apache Hive and Apache Spark. For these reasons, multiple studies have proposed new techniques and evaluated the processing of XML files with Big Data systems. However, a more usual approach in such works involves the simplest XML schemas, even though, real data sets are composed of complex schemas. Therefore, to shed light on complex XML schema processing for real-life applications with Big Data tools, we present an approach that combines  three techniques. This comprises three main methods for parsing XML files: cataloging, deserialization, and positional explode. For cataloging, the elements of the XML schema are mapped into root, arrays, structures, values, and attributes. Based on these elements, the deserialization and positional explode are straightforwardly implemented. To demonstrate the validity of our proposal, we develop a case study by implementing a test environment to illustrate the methods using real data sets provided from performance management of two mobile network vendors. Our main results state the validity of the proposed method for different versions of Apache Hive and Apache Spark, obtain the query execution times for Apache Hive internal and external tables and Apache Spark data frames,  and compare the query performance in Apache Hive with that of Apache Spark. Another contribution made is a case study in which a novel solution is proposed for data analysis in the performance management systems of mobile networks.;;Martinez-Mosquera D, Navarrete R, Lujan-Mora S;;eng;;['Complex XSD', 'Hive', 'Mobile network', 'Performance management', 'Spark', 'XML'];The authors declare there are no competing interests.;;10.7717/peerj-cs.652
17;34418958;Article;2021;BMC medical research methodology;;Use of research electronic data capture (REDCap) in a COVID-19 randomized controlled trial: a practical example.;;"BACKGROUND: Randomized controlled trials (RCT) are considered the ideal design for evaluating the efficacy of interventions. However, conducting a successful RCT has technological and logistical challenges. Defects in randomization processes (e.g., allocation sequence concealment) and flawed masking could bias an RCT's findings. Moreover, investigators need to address other logistics common to all study designs, such as study invitations, eligibility screening, consenting procedure, and data confidentiality protocols. Research Electronic Data Capture (REDCap) is a secure, browser-based web application widely used by researchers for survey data collection. REDCap offers unique features that can be used to conduct rigorous RCTs. METHODS: In September and November 2020, we conducted a parallel group RCT among Indiana University Bloomington (IUB) undergraduate students to understand if receiving the results of a SARS-CoV-2 antibody test changed the students' self-reported protective behavior against coronavirus disease 2019 (COVID-19). In the current report, we discuss how we used REDCap to conduct the different components of this RCT. We further share our REDCap project XML file and instructional videos that investigators can use when  designing and conducting their RCTs. RESULTS: We reported on the different features that REDCap offers to complete various parts of a large RCT, including sending study invitations and recruitment, eligibility screening, consenting procedures, lab visit appointment and reminders, data collection and confidentiality, randomization, blinding of treatment arm assignment, returning test results, and follow-up surveys. CONCLUSIONS: REDCap offers powerful tools for longitudinal data collection and conduct of rigorous and successful RCTs. Investigators can make use of this electronic data capturing system to successfully complete their RCTs. TRIAL REGISTRATION: The RCT was prospectively (before completing data collection) registered at ClinicalTrials.gov; registration number: NCT04620798 , date of registration: November 9, 2020.";;Kianersi S, Luetke M, Ludema C, Valenzuela A, Rosenberg M;;eng;['*COVID-19', 'Electronics', 'Humans', 'Randomized Controlled Trials as Topic', '*Research Design', 'SARS-CoV-2', 'Surveys and Questionnaires'];['*RCT', '*REDCap', '*Randomization', '*Randomized controlled trials', '*Risk of bias'];;;10.1186/s12874-021-01362-2
18;34344385;Article;2021;BMC medical informatics and decision making;;COVID term: a bilingual terminology for COVID-19.;;"BACKGROUND: The coronavirus disease (COVID-19), a pneumonia caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has shown its destructiveness with more than one million confirmed cases and dozens of thousands of death, which is highly contagious and still spreading globally. World-wide studies have been conducted aiming to understand the COVID-19 mechanism, transmission, clinical features, etc. A cross-language terminology of  COVID-19 is essential for improving knowledge sharing and scientific discovery dissemination. METHODS: We developed a bilingual terminology of COVID-19 named COVID Term with mapping Chinese and English terms. The terminology was constructed as follows: (1) Classification schema design; (2) Concept representation model building; (3) Term source selection and term extraction; (4) Hierarchical structure construction; (5) Quality control (6) Web service. We built open access for the terminology, providing search, browse, and download services. RESULTS: The proposed COVID Term include 10 categories: disease, anatomic site, clinical manifestation, demographic and socioeconomic characteristics, living organism, qualifiers, psychological assistance, medical equipment, instruments and materials, epidemic prevention and control, diagnosis  and treatment technique respectively. In total, COVID Terms covered 464 concepts  with 724 Chinese terms and 887 English terms. All terms are openly available online (COVID Term URL: http://covidterm.imicams.ac.cn ). CONCLUSIONS: COVID Term is a bilingual terminology focused on COVID-19, the epidemic pneumonia with a high risk of infection around the world. It will provide updated bilingual terms  of the disease to help health providers and medical professionals retrieve and exchange information and knowledge in multiple languages. COVID Term was released in machine-readable formats (e.g., XML and JSON), which would contribute to the information retrieval, machine translation and advanced intelligent techniques application.";;Ma H, Shen L, Sun H, Xu Z, Hou L, Wu S, Fang A, Li J, Qian Q;;eng;['*COVID-19', '*Epidemics', 'Humans', 'Information Storage and Retrieval', 'Language', 'SARS-CoV-2'];['*Bilingual', '*COVID-19', '*Medical terminology', '*Terminology system'];;2016YFC0901901/National Key Research and Development Program of China;10.1186/s12911-021-01593-9
19;34306613;Article;2021;Ecology and evolution;;Metadata Made Easy: Develop and Use Domain-Specific Metadata Schemes by following the dmdScheme approach.;;Metadata plays an essential role in the long-term preservation, reuse, and interoperability of data. Nevertheless, creating useful metadata can be sufficiently difficult and weakly enough incentivized that many datasets may be accompanied by little or no metadata. One key challenge is, therefore, how to make metadata creation easier and more valuable. We present a solution that involves creating domain-specific metadata schemes that are as complex as necessary and as simple as possible. These goals are achieved by co-development between a metadata expert and the researchers (i.e., the data creators). The final product is a bespoke metadata scheme into which researchers can enter information (and validate it) via the simplest of interfaces: a web browser application and a spreadsheet.We provide the R package dmdScheme (dmdScheme: An R package for working with domain specific MetaData schemes (Version v0.9.22), 2019) for creating a template domain-specific scheme. We describe how to create a domain-specific scheme from this template, including the iterative co-development process, and the simple methods for using the scheme, and simple methods for quality assessment, improvement, and validation.The process of developing a metadata scheme following the outlined approach was successful, resulting in a metadata scheme which is used for the data generated in our research group. The validation quickly identifies forgotten metadata, as well as inconsistent metadata, therefore improving the quality of the metadata. Multiple output formats are available, including XML.Making the provision of metadata easier while also ensuring high quality must be a priority for data curation initiatives. We show how both objectives are achieved by close collaboration between metadata experts and researchers to create domain-specific schemes. A near-future priority is to provide methods to interface domain-specific schemes with general metadata schemes, such as the Ecological Metadata Language, to increase interoperability.;;Krug RM, Petchey OL;;eng;;;The authors declare that they have no conflict of interest.;;10.1002/ece3.7764
20;34304211;Article;2021;Indian journal of ophthalmology;;Intricate scientometric analysis and citation trend of COVID-19-related publications in Indian Journal of Ophthalmology during COVID-19 pandemic.;;Purpose: To analyze the trend of COVID-19-related publications in the Indian Journal of Ophthalmology (IJO) and assess the specialty wise correlation, distribution, and citation trend during the COVID-19 pandemic. Methods: A retrospective analysis of all COVID-19-related articles was performed from April  2020 to May 2021. The bibliographic records were obtained from the website of IJO, Editor IJO email, and PubMed. The data was then exported as XML into Microsoft access for scientometric analysis. The articles were segregated as Original, Review, Case Report/Series, Letter to the Editor/Commentary, Guest Editorial, PointCounterpoint, Consensus Criteria, Ophthalmic Images, Photo Essay, Surgical Techniques, and All India Ophthalmic Society Meeting Papers. The data was comprehensively analyzed for specialty-wise correlations, distribution, citation trend, and reasons for the same. Results: A total of 231 COVID-19-related articles were published during the study period. The maximum articles were [82 (35.49%)] letters to the editor, followed by [51 (22.08%)] original articles, [30 (12.99%)] commentaries, and [20 (8.66%)] editorials. The least were perspectives, consensus, images, and photo assay with [1 (0.43%)] each. The maximum publications were in July [44 (19.05%)] and least in April [1 (0.43%)]. Considering specialty, the maximum articles were related to general ophthalmology [124 (53.68%)] and least were in refractive surgery and community ophthalmology with [1 (0.43%)] each. The maximum citations were for original articles [352 (34.65%)], which was 2.3 times higher than review articles and letters to editor [150 (14.76%)]. General ophthalmology had 740 (72.83%) citations, which were nearly five times that of cornea [140 (13.78%)]. Conclusion: The IJO showed a trough and crest pattern of COVID-19 publications month wise. Letter to editor and general ophthalmology COVID-19 articles had maximum publications with maximum citations for general ophthalmology owing to practice patterns and COVID-19 challenges.;;Kaur K, Gurnani B;;eng;['*COVID-19', 'Humans', 'India/epidemiology', '*Ophthalmology', 'Pandemics', 'Retrospective Studies', 'SARS-CoV-2'];['*COVID-19', '*COVID-19 pandemic', '*Citation trend', '*Indian Journal of Ophthalmology (IJO)', '*scientometric analysis'];None;;10.4103/ijo.IJO_829_21
21;34295233;Article;2021;Frontiers in neuroinformatics;;MIIND : A Model-Agnostic Simulator of Neural Populations.;;MIIND is a software platform for easily and efficiently simulating the behaviour  of interacting populations of point neurons governed by any 1D or 2D dynamical system. The simulator is entirely agnostic to the underlying neuron model of each population and provides an intuitive method for controlling the amount of noise which can significantly affect the overall behaviour. A network of populations can be set up quickly and easily using MIIND's XML-style simulation file format describing simulation parameters such as how populations interact, transmission delays, post-synaptic potentials, and what output to record. During simulation, a visual display of each population's state is provided for immediate feedback of the behaviour and population activity can be output to a file or passed to a Python script for further processing. The Python support also means that MIIND can be integrated into other software such as The Virtual Brain. MIIND's population density technique is a geometric and visual method for describing the  activity of each neuron population which encourages a deep consideration of the dynamics of the neuron model and provides insight into how the behaviour of each  population is affected by the behaviour of its neighbours in the network. For 1D  neuron models, MIIND performs far better than direct simulation solutions for large populations. For 2D models, performance comparison is more nuanced but the  population density approach still confers certain advantages over direct simulation. MIIND can be used to build neural systems that bridge the scales between an individual neuron model and a population network. This allows researchers to maintain a plausible path back from mesoscopic to microscopic scales while minimising the complexity of managing large numbers of interconnected neurons. In this paper, we introduce the MIIND system, its usage,  and provide implementation details where appropriate.;;Osborne H, Lai YM, Lepperod ME, Sichau D, Deutz L, de Kamps M;;eng;;['GPU', 'Python', 'dynamical systems', 'network', 'neural population', 'population density', 'simulator', 'software'];The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.;;10.3389/fninf.2021.614881
22;34288531;Article;2022;Molecular ecology resources;;SPART: A versatile and standardized data exchange format for species partition information.;;"A wide range of data types can be used to delimit species and various computer-based tools dedicated to this task are now available. Although these formalized approaches have significantly contributed to increase the objectivity  of species delimitation (SD) under different assumptions, they are not routinely  used by alpha-taxonomists. One obvious shortcoming is the lack of interoperability among the various independently developed SD programs. Given the frequent incongruences between species partitions inferred by different SD approaches, researchers applying these methods often seek to compare these alternative species partitions to evaluate the robustness of the species boundaries. This procedure is excessively time consuming at present, and the lack of a standard format for species partitions is a major obstacle. Here, we propose a standardized format, SPART, to enable compatibility between different SD tools  exporting or importing partitions. This format reports the partitions and describes, for each of them, the assignment of individuals to the ""inferred species"". The syntax also allows support values to be optionally reported, as well as original trees and the full command lines used in the respective SD analyses. Two variants of this format are proposed, overall using the same terminology but presenting the data either optimized for human readability (matricial SPART) or in a format in which each partition forms a separate block (SPART.XML). ABGD, DELINEATE, GMYC, PTP and TR2 have already been adapted to output SPART files and a new version of LIMES has been developed to import, export, merge and split them.";;Miralles A, Ducasse J, Brouillet S, Flouri T, Fujisawa T, Kapli P, Knowles LL, Kumari S, Stamatakis A, Sukumaran J, Lutteropp S, Vences M, Puillandre N;;eng;;['LIMES v2.0', 'SPART', 'integrative taxonomy', 'species delimitation programs', 'species partition format'];;865101/H2020 European Research Council;10.1111/1755-0998.13470
23;34285675;Article;2021;Journal of the Medical Library Association : JMLA;;Updating search strategies for literature reviews with OUR2D2: an open-source computer application.;;Background: While writing a scoping review, we needed to update our search strategy. We wanted to capture articles generated by our additional search terms  and articles published since our original search. Simultaneously, we strove to optimize project resources by not rescreening articles that had been captured in  our original results. Case presentation: In response, we created Open Update Re-run Deduplicate (OUR2D2), a computer application that allows the user to compare search results from a variety of library databases. OUR2D2 supports extensible markup language (XML) files from EndNote and comma-separated values (CSV) files using article titles for comparisons. We conducted unit tests to ensure appropriate functionality as well as accurate data extraction and analysis. We tested OUR2D2 by comparing original and updated search results from  PubMed, Embase, Clarivate Web of Science, CINAHL, Scopus, ProQuest Dissertation and Theses, and Lens and estimate that this application saved twenty-one hours of work during the screening process. Conclusions: OUR2D2 could be useful for individuals seeking to update literature review strategies across fields without  rescreening articles from previous searches. Because the OUR2D2 source code is freely available with a permissive license, we recommend this application for researchers conducting literature reviews who need to update their search results over time, want a powerful and flexible analysis framework, and may not have access to paid subscription tools.;;Lohr AM, Van Gorden N, McClelland DJ, Dubinsky E, Gerald LB, Wilkinson-Lee A, Carvajal SC;;eng;['*Computers', 'Humans', 'PubMed', '*Software'];['collaborative work', 'literature search', 'open access'];;;10.5195/jmla.2021.1105
24;34247250;Article;2021;Journal of radiation research;;Survey of X-ray induced Cherenkov excited fluorophores with potential for human use.;;X-ray induced molecular luminescence (XML) is a phenomenon that can be utilized for clinical, deep-tissue functional imaging of tailored molecular probes. In this study, a survey of common or clinically approved fluorophores was carried out for their megavoltage X-ray induced excitation and emission characteristics.  We find that direct scintillation effects and Cherenkov generation are two possible ways to cause these molecules' excitation. To distinguish the contributions of each excitation mechanism, we exploited the dependency of Cherenkov radiation yield on X-ray energy. The probes were irradiated by constant dose of 6 MV and 18 MV X-ray radiation, and their relative emission intensities and spectra were quantified for each X-ray energy pair. From the ratios of XML, yield for 6 MV and 18 MV irradiation we found that the Cherenkov radiation dominated as an excitation mechanism, except for aluminum phthalocyanine, which exhibited substantial scintillation. The highest emission yields were detected from fluorescein, proflavin and aluminum phthalocyanine, in that order. XML yield was found to be affected by the emission quantum yield, overlap of the fluorescence excitation and Cherenkov emission spectra, scintillation yield. Considering all these factors and XML emission spectrum respective to tissue optical window, aluminum phthalocyanine offers the best XML yield for deep tissue use, while fluorescein and proflavine are most useful for subcutaneous or superficial use.;;Petusseau AF, Bruza P, Pogue BW;;eng;['Drug Evaluation, Preclinical', 'Equipment Design', 'Fluorescein/radiation effects', 'Fluorescent Dyes/*radiation effects', 'Humans', 'Indoles/radiation effects', 'Isoindoles/radiation effects', '*Luminescence', 'Methylene Blue/radiation effects', 'Organometallic Compounds/radiation effects', 'Particle Accelerators', 'Proflavine/radiation effects', 'Protoporphyrins/radiation effects', 'Solvents', 'Spectrometry, Fluorescence', 'Verteporfin/radiation effects', 'X-Rays'];['Cherenkov radiation', 'X-ray induced molecular luminescence (XML)', 'organic fluorophores', 'scintillation', 'therapeutic megavoltage X-rays'];;R01 EB024498/EB/NIBIB NIH HHS/United States;10.1093/jrr/rrab055
25;34225374;Article;2021;Methods of information in medicine;;MAGICPL: A Generic Process Description Language for Distributed Pseudonymization  Scenarios.;;OBJECTIVES: Pseudonymization is an important aspect of projects dealing with sensitive patient data. Most projects build their own specialized, hard-coded, solutions. However, these overlap in many aspects of their functionality. As any  re-implementation binds resources, we would like to propose a solution that facilitates and encourages the reuse of existing components. METHODS: We analyzed already-established data protection concepts to gain an insight into their common features and the ways in which their components were linked together. We found that we could represent these pseudonymization processes with a simple descriptive language, which we have called MAGICPL, plus a relatively small set of components. We designed MAGICPL as an XML-based language, to make it human-readable and accessible to nonprogrammers. Additionally, a prototype implementation of the components was written in Java. MAGICPL makes it possible to reference the components using their class names, making it easy to extend or  exchange the component set. Furthermore, there is a simple HTTP application programming interface (API) that runs the tasks and allows other systems to communicate with the pseudonymization process. RESULTS: MAGICPL has been used in  at least three projects, including the re-implementation of the pseudonymization  process of the German Cancer Consortium, clinical data flows in a large-scale translational research network (National Network Genomic Medicine), and for our own institute's pseudonymization service. CONCLUSIONS: Putting our solution into  productive use at both our own institute and at our partner sites facilitated a reduction in the time and effort required to build pseudonymization pipelines in  medical research.;;Tremper G, Brenner T, Stampe F, Borg A, Bialke M, Croft D, Schmidt E, Lablans M;;eng;['*Biomedical Research', 'Computer Security', 'Confidentiality', 'Humans', '*Language', 'Software'];;None declared.;;10.1055/s-0041-1731387
26;34090460;Article;2021;Human resources for health;;Data element mapping to analyze fit for use of three XML standards for health workforce tracking.;;BACKGROUND: Ensuring a sufficient supply and distribution of health care professionals is essential to meeting public health needs. Regulatory agencies protect their communities by ensuring that new health professionals have the required qualifications to practice safely and by tracking the volume and distribution of those professionals on an ongoing basis. The speed and accuracy of sharing these data could be greatly improved through the adoption of a data standard for information about health professionals. To date, however, no internationally accepted standard has emerged for this purpose. PURPOSE: This study examines three existing XML standards designed for the representation of individual worker data to determine if, and to what degree, each could be used for the tracking of health professionals. METHODS: The data elements of the Europass schema, the HR Open Standard Recruiting specification, and the MedBiquitous Healthcare Professional Profile standard were fully examined and matching elements were mapped to the 200+ elements identified from a prior content analysis as required by a sample of 20 international regulatory agencies. RESULTS: None of the schemas examined addressed more than half of the information elements required by regulators. All three schemas are found lacking in some key  areas of interest, especially vital information that could disqualify ineligible  applicant practitioners. CONCLUSIONS: The three standards could all be improved by including new elements essential to regulatory agencies. Regulatory agencies should be consulted in the development of new standards for representing potentially disqualifying information about candidates for professional practice.;;Opalek A;;eng;['*Health Personnel', '*Health Workforce', 'Humans', 'Public Health', 'Referral and Consultation'];['*Health HRIS', '*Health workforce planning', '*International data standard', '*Interoperability', '*Professional regulation'];;;10.1186/s12960-021-00615-x
27;34055023;Article;2021;Evidence-based complementary and alternative medicine : eCAM;;Systems Pharmacology and In Silico Docking Analysis Uncover Association of CA2, PPARG, RXRA, and VDR with the Mechanisms Underlying the Shi Zhen Tea Formula Effect on Eczema.;;Eczema is a complex chronic inflammatory skin disease impacted by environmental factors, infections, immune disorders, and deficiencies in skin barrier function. Shi Zhen Tea (SZT), derived from traditional Chinese medicine Xiao-Feng-San, has  shown to be an effective integrative therapy for treating skin lesions, itching,  and sleeping loss, and it facilitates reduction of topical steroid and antihistamine use in pediatric and adult patients with severe eczema. Yet, its active compounds and therapeutic mechanisms have not been elucidated. In this study, we sought to investigate the active compounds and molecular mechanisms of  SZT in treating eczema using systems pharmacology and in silico docking analysis. SZT is composed of 4 medicinal herbs, Baizhu (Atractylodis macrocephalae rhizome), Jingjie (Schizonepetae herba), Kushen (Sophorae flavescentis radix), and Niubangzi (Arctii fructus). We first identified 51 active compounds from SZT  and their 81 potential molecular targets by high-throughput computational analysis, from which we identified 4 major pathways including Th17 cell differentiation, metabolic pathways, pathways in cancer, and the PI3K-Akt signaling pathway. Through network analysis of the compound-target pathway, we identified hub molecular targets within these pathways including carbonic anhydrase II (CA2), peroxisome proliferator activated receptor gamma (PPAR gamma), retinoid X receptor alpha (RXRA), and vitamin D receptor (VDR). We further identified top 5 compounds including cynarine, stigmasterin, kushenol, beta-sitosterol, and (24S)-24-propylcholesta-5-ene-3beta-ol as putative key active compounds on the basis of their molecular docking scores with identified hub target proteins. Our study provides an insight into the therapeutic mechanism underlying multiscale benefits of SZT for eczema and paves the way for developing new and potentially more effective eczema therapies.;;Wang ZZ, Jia Y, Srivastava KD, Huang W, Tiwari R, Nowak-Wegrzyn A, Geliebter J, Miao M, Li XM;;eng;;;"ZZW, YL, WH, RT, ANW, JG, and MM have no financial conflicts of interest to disclose. KDS receives salary from General Nutraceutical Technology LLC. XML received research support from the National Institutes of Health (NIH)/National Center for Complementary and Alternative Medicine (NCCAM), Food Allergy Research  and Education (FARE) and Winston Wolkoff Integrative Medicine Fund for Allergies  and Wellness, the Parker Foundation, and Henan University of Chinese Medicine; received consulting fees from Food Allergy Research and Education (FARE), Johnson & Johnson Pharmaceutical Research & Development, LLC, and Bayer Global Health LLC; royalties from UpToDate; travel expenses from the National Center for Complementary and Alternative Medicine (NCCAM) and FARE, Henan University of Chinese Medicine, Harvard TCM summit, China Allergy Society Conference, and North American TCM Symposium; shares US patents PCT/US05/008417, PCT/US 10,500,169, PCP/14/762,416, and PCP 14/762,416; is a member of Herbs Springs, LLC, General Nutraceutical Technology LLC, and Health Freedom LLC; and takes compensation from practice at Integrative Health and Acupuncture PC.";;10.1155/2021/8406127
28;34042880;Article;2021;Studies in health technology and informatics;;openEHR Mapper - A Tool to Fuse Clinical and Genomic Data Using the openEHR Standard.;;Precision medicine is an emerging and important field for health care. Molecular  tumor boards use a combination of clinical and molecular data, such as somatic tumor mutations to decide on personalized therapies for patients who have run out of standard treatment options. Personalized treatment decisions require clinical  data from the hospital information system and mutation data to be accessible in a structured way. Here we introduce an open data platform to meet these requirements. We use the openEHR standard to create an expert-curated data model  that is stored in a vendor-neutral format. Clinical and molecular patient data is integrated into cBioPortal, a warehousing solution for cancer genomic studies that is extended for use in clinical routine for molecular tumor boards. For data integration, we developed openEHR Mapper, a tool that allows to (i) process input data, (ii) communicate with the openEHR repository, and (iii) export the data to  cBioPortal. We benchmarked the mapper performance using XML and JSON as serialization format and added caching capabilities as well as multi-threading to the openEHR Mapper.;;Reimer N, Ulrich H, Busch H, Kock-Schoppenhauer AK, Ingenerf J;;eng;['*Electronic Health Records', '*Genomics', 'Humans', 'Software'];['Health Information Interoperability', 'HiGHmed', 'cBioPortal', 'openEHR', 'tumor board'];;;10.3233/SHTI210055
29;34042663;Article;2021;Studies in health technology and informatics;;Using QR Code for Uniform Representation of Content in Cross-Border Exchange of ePrescriptions in the EU.;;This paper proposes an approach and demonstrates its application for cross-border exchange of ePrescriptions in the European Union. A business process model of the main use case for exchange of prescription content in the eHealth Digital Service Infrastructure is created and analyzed. The novelty in this approach is the proposed encoding of the basic dataset in a Quick Response (QR) code in terms of  an XML scheme that is independent of clinical models or proprietary database structures. It allows to inverse the dataflow control in the chain of message exchanges between Dispenser and National Contact Points. The proposed inversion of control positions the citizen with the QR code of the prescription in the center of that chain of message exchanges between the main actors of the business process. The independent format of content representation in the QR code allows the actors in the message exchange to auto-populate data in their registers when  the medicine is dispensed. Initial results are reported and reveal the advantages of embedding prescription details in QR code employing a common independent XML scheme.;;Krastev E, Kovachev P, Tcharaktchiev D, Abanos S;;eng;['Databases, Factual', 'European Union', '*Telemedicine'];['QR code', 'XML', 'business process model and notation', 'cross-border exchange', 'ePrescription', 'health informatics', 'inversion of control'];;;10.3233/SHTI210259
30;34034730;Article;2021;Microbial cell factories;;Stepwise metabolic engineering of Candida tropicalis for efficient xylitol production from xylose mother liquor.;;BACKGROUND: Commercial xylose purification produces xylose mother liquor (XML) as a major byproduct, which has become an inexpensive and abundant carbon source. A  portion of this XML has been used to produce low-value-added products such as caramel but the remainder often ends up as an organic pollutant. This has become  an issue of industrial concern. In this study, a uracil-deficient Candida tropicalis strain was engineered to efficiently convert XML to the commercially useful product xylitol. RESULTS: The xylitol dehydrogenase gene was deleted to block the conversion of xylitol to xylulose. Then, an NADPH regeneration system was added through heterologous expression of the Yarrowia lipolytica genes encoding 6-phosphate-gluconic acid dehydrogenase and 6-phosphate-glucose dehydrogenase. After process optimization, the engineered strain, C. tropicalis XZX-B4ZG, produced 97.10 g L(- 1) xylitol in 120 h from 300 g L(- 1) XML in a 5-L fermenter. The xylitol production rate was 0.82 g L(- 1 )h(- 1) and the conversion rate was 92.40 %. CONCLUSIONS: In conclusion, this study performed a combination of metabolic engineering and process optimizing in C. tropicalis to enhance xylitol production from XML. The use of C. tropicalis XZX-B4ZG, therefore, provided a convenient method to transform the industrial by-product XML into the useful material xylitol.;;Zhang L, Chen Z, Wang J, Shen W, Li Q, Chen X;;eng;['Candida tropicalis/enzymology/*genetics/*metabolism', 'D-Xylulose Reductase/*genetics/metabolism', 'Fermentation', 'Glucose 1-Dehydrogenase', 'Glucosephosphate Dehydrogenase/metabolism', 'Industrial Microbiology', '*Metabolic Engineering', 'Xylitol/*biosynthesis', 'Xylose/*metabolism'];['Candida tropicalis', 'Cofactor regeneration', 'Fermentation optimization', 'Xylitol', 'Xylose mother liquor'];;BK20171138/Natural Science Foundation of Jiangsu Province;10.1186/s12934-021-01596-1
31;33980685;Article;2021;BMJ (Clinical research ed.);;<xml><?covid-license?></xml>Airborne transmission of covid-19: reduce the viral load in inhaled air.;;;;Walker GJ, Foster S;;eng;['*COVID-19', 'Humans', 'SARS-CoV-2', 'Serologic Tests', 'Viral Load'];;Competing interests: None declared.;;10.1136/bmj.n1198
32;33961037;Article;2021;JAMA network open;;Comparison of Treatments for Cocaine Use Disorder Among Adults: A Systematic Review and Meta-analysis.;;"Importance: In the US and the United Kingdom, cocaine use is the second leading cause of illicit drug overdose death. Psychosocial treatments for cocaine use disorder are limited, and no pharmacotherapy is approved for use in the US or Europe. Objective: To compare treatments for active cocaine use among adults. Data Sources: PubMed and the Cochrane Database of Systematic Reviews were searched for clinical trials published between December 31, 1995, and December 31, 2017. Study Selection: This meta-analysis was registered on Covidence.org (study 8731) on December 31, 2015. Clinical trials were included if they (1) had  the term cocaine in the article title; (2) were published between December 31, 1995, and December 31, 2017; (3) were written in English; (4) enrolled outpatients 18 years or older with active cocaine use at baseline; and (5) reported treatment group size, treatment duration, retention rates, and urinalysis results for the presence of cocaine metabolites. A study was excluded  if (1) more than 25% of participants were not active cocaine users or more than 80% of participants had negative test results for the presence of cocaine metabolites at baseline and (2) it reported only pooled urinalysis results indicating the presence of multiple substances and did not report the specific proportion of positive test results for cocaine metabolites. Multiple reviewers reached criteria consensus. Of 831 records screened, 157 studies (18.9%) met selection criteria and were included in the analysis. Data Extraction and Synthesis: This study followed the Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) guideline. Search results were imported from PubMed XML into Covidence.org then Microsoft Excel. Data extraction was completed in 2 iterations to ensure fidelity. Analyses included a multilevel random-effects model, a multilevel mixed-effects meta-regression model, and sensitivity analyses. Treatments were clustered into 11 categories (psychotherapy, contingency management programs, placebo, opioids, psychostimulants, anticonvulsants, dopamine agonists, antidepressants, antipsychotics, miscellaneous medications, and other therapies). Missing data were imputed using  multiple imputation by chained equations. The significance threshold for all analyses was P = .05. Data were analyzed using the metafor and mice packages in R software, version 3.3.2 (R Foundation for Statistical Computing). Data were analyzed from January 1, 2018, to February 28, 2021. Main Outcomes and Measures:  The primary outcome was the intention-to-treat logarithm of the odds ratio (OR) of having a negative urinalysis result for the presence of cocaine metabolites at the end of each treatment period compared with baseline. The hypothesis, which was formulated after data collection, was that no treatment category would have a significant association with objective reductions in cocaine use. Results: A total of 157 studies comprising 402 treatment groups and 15842 participants were  included. Excluding other therapies, the largest treatment groups across all studies were psychotherapy (mean [SD] number of participants, 40.04 [36.88]) and  contingency management programs (mean [SD] number of participants, 37.51 [25.51]). Only contingency management programs were significantly associated with an increased likelihood of having a negative test result for the presence of cocaine (OR, 2.13; 95% CI, 1.62-2.80), and this association remained significant  in all sensitivity analyses. Conclusions and Relevance: In this meta-analysis, contingency management programs were associated with reductions in cocaine use among adults. Research efforts and policies that align with this treatment modality may benefit those who actively use cocaine and attenuate societal burdens.";;Bentzley BS, Han SS, Neuner S, Humphreys K, Kampman KM, Halpern CH;;eng;['Clinical Trials as Topic', 'Cocaine-Related Disorders/*therapy', 'Humans', 'Psychotherapy'];;;;10.1001/jamanetworkopen.2021.8049
33;33954304;Article;2021;Journal of immunological sciences;;Implementation of Mobile Phone Data Collection in the Conduct EPI Comprehensive Review in East and Southern African Countries.;;Mobile phone data collection tools are increasingly becoming very usable collecting, collating and analysing data in the health sector. In this paper, we  documented the experiences with mobile phone data collection, collation and analysis in 5 countries of the East and Southern African, using Open Data Kit (ODK), where questionnaires were designed and coded on an XML form, uploaded and  data collected using Android-Based mobile phones, with a web-based system to monitor data in real-time during EPI comprehensive review. The ODK interface supports in real-time monitoring of the flow of data, detection of missing or incomplete data, coordinate location of all locations visited, embedded charts for basic analysis. It also minimized data quality errors at entry level with the use of validation codes and constraint developed into the checklist. These benefits, combined with the improvement that mobile phones offer over paper-based in terms of timeliness, data loss, collation, and real-time data collection, analysis and uploading difficulties, make mobile phone data collection a feasible method of data collection that needs to be further explored in the conduct of all surveys in the organization.;;Bello IM, Umar AS, Akpan GU, Okeibunor J, Shibeshi C, Eshetu M, Magwati CJ, Fasil T, Fussum D, Mihigo R, Mkanda P;;eng;;;;001/WHO_/World Health Organization/International;10.29245/2578-3009/2021/S2.1108
34;33954235;Article;2021;PeerJ. Computer science;;LOPDF: a framework for extracting and producing open data of scientific documents for smart digital libraries.;;Background: Results of scientific experiments and research work, either conducted by individuals or organizations, are published and shared with scientific community in different types of scientific publications such as books, chapters,  journals, articles, reference works and reference works entries. One aspect of these documents is their contents and the other is metadata. Metadata of scientific documents could be used to increase mutual cooperation, find people with common interest and research work, and to find scientific documents in the matching domains. The major issue in getting these benefits from metadata of scientific publications is availability of these data in unstructured (or semi-structured) format so that it can not be used to ask smart queries that can  help in computing and performing different types of analysis on scientific publications data. Also, acquisition and smart processing of publications data is a complicated as well as time and resource consuming task. Methods: To address this problem we have developed a generic framework named as Linked Open Publications Data Framework (LOPDF). The LOPDF framework can be used to crawl, process, extract and produce machine understandable data (i.e., LOD) about scientific publications from different publisher specific sources such as portals, XML export and websites. In this paper we present the architecture, process and algorithm that we developed to process textual publications data and  to produce semantically enriched data as RDF datasets (i.e., open data). Results: The resulting datasets can be used to make smart queries by making use of SPARQL  protocol. We also present the quantitative as well as qualitative analysis of our resulting datasets which ultimately can be used to compute the research behavior  of organizations in rapidly growing knowledge society. Finally, we present the potential usage of producing and processing such open data of scientific publications and how results of performing smart queries on resulting open datasets can be used to compute the impact and perform different types of analysis on scientific publications data.;;Aslam MA;;eng;;['Algorithms analysis', 'Digital libraries', 'Ontological reasoning', 'Open data'];The authors declare there are no competing interests.;;10.7717/peerj-cs.445
35;33930535;Article;2021;Journal of biomedical informatics;;"Genomic considerations for FHIR(R); eMERGE implementation lessons.";;Structured representation of clinical genetic results is necessary for advancing  precision medicine. The Electronic Medical Records and Genomics (eMERGE) Network's Phase III program initially used a commercially developed XML message format for standardized and structured representation of genetic results for electronic health record (EHR) integration. In a desire to move towards a standard representation, the network created a new standardized format based upon Health Level Seven Fast Healthcare Interoperability Resources (HL7(R) FHIR(R)), to represent clinical genomics results. These new standards improve the utility of HL7(R) FHIR(R) as an international healthcare interoperability standard for management of genetic data from patients. This work advances the establishment of standards that are being designed for broad adoption in the current health information technology landscape.;;Murugan M, Babb LJ, Overby Taylor C, Rasmussen LV, Freimuth RR, Venner E, Yan F, Yi V, Granite SJ, Zouk H, Aronson SJ, Power K, Fedotov A, Crosslin DR, Fasel D, Jarvik GP, Hakonarson H, Bangash H, Kullo IJ, Connolly JJ, Nestor JG, Caraballo PJ, Wei W, Wiley K, Rehm HL, Gibbs RA;;eng;['*Electronic Health Records', 'Genomics', 'Health Level Seven', 'Humans', '*Medical Informatics', 'Precision Medicine'];['*Clinical decision support', '*Clinical genomics', '*Electronic health record (EHR)', '*Genetic test results', '*HL7(R) FHIR(R) standard', '*Interoperability'];;U01 HG008676/HG/NHGRI NIH HHS/United States;10.1016/j.jbi.2021.103795
36;33854694;Article;2021;Oxidative medicine and cellular longevity;;Xinmailong Attenuates Doxorubicin-Induced Lysosomal Dysfunction and Oxidative Stress in H9c2 Cells via HO-1.;;The clinical use of doxorubicin (DOX) is limited by its cardiotoxicity, which is  closely associated with oxidative stress. Xinmailong (XML) is a bioactive peptide extracted from American cockroaches, which has been mainly applied to treat chronic heart failure in China. Our previous study showed that XML attenuates DOX-induced oxidative stress. However, the mechanism of XML in DOX-induced cardiotoxicity remains unclear. Heme oxygenase-1 (HO-1), an enzyme that is ubiquitously expressed in all cell types, has been found to take antioxidant effects in many cardiovascular diseases, and its expression is protectively upregulated under DOX treatment. Lysosome and autophagy are closely involved in oxidative stress as well. It is still unknown whether XML could attenuate doxorubicin-induced lysosomal dysfunction and oxidative stress in H9c2 cells via  HO-1. Thus, this study was aimed at investigating the involvement of HO-1-mediated lysosomal function and autophagy flux in DOX-induced oxidative stress and cardiotoxicity in H9c2 cells. Our results showed that XML treatment markedly increased cell proliferation and SOD activity, improved lysosomal function, and ameliorated autophagy flux block in DOX-treated H9c2 cells. Furthermore, XML significantly increased HO-1 expression following DOX treatment. Importantly, HO-1-specific inhibitor (Znpp) or HO-1 siRNA could significantly attenuate the protective effects of XML against DOX-induced cell injury, oxidative stress, lysosomal dysfunction, and autophagy flux block. These results  suggest that XML protects against DOX-induced cardiotoxicity through HO-1-mediated recovery of lysosomal function and autophagy flux and decreases oxidative stress, providing a novel mechanism responsible for the protection of XML against DOX-induced cardiomyopathy.;;Jiang Y, Liu Y, Xiao W, Zhang D, Liu X, Xiao H, You S, Yuan L;;eng;['Animals', 'Antibiotics, Antineoplastic/adverse effects/pharmacology', 'Cell Line', 'Doxorubicin/adverse effects/*pharmacology', 'Drug Interactions', 'Drugs, Chinese Herbal/*pharmacology', 'Heme Oxygenase (Decyclizing)/antagonists & inhibitors/*metabolism', 'Lysosomes/*drug effects/enzymology', 'Myocytes, Cardiac/*drug effects/enzymology', 'Oxidative Stress/*drug effects', 'Rats'];;The authors declare no conflicts of interest regarding the publication of this article.;;10.1155/2021/5896931
37;33839303;Article;2021;Journal of biomedical informatics;;CDISC-compliant clinical trial imaging management system with automatic verification and data Transformation: Focusing on tumor response assessment data  in clinical trials.;;OBJECTIVE: Major issues in imaging data management of tumor response assessment in clinical trials include high human errors in data input and unstandardized data structures, warranting a new breakthrough IT solution. Thus, we aim to develop a Clinical Data Interchange Standards Consortium (CDISC)-compliant clinical trial imaging management system (CTIMS) with automatic verification and  transformation modules for implementing the CDISC Study Data Tabulation Model (SDTM) in the tumor response assessment dataset of clinical trials. MATERIALS AND METHODS: In accordance with various CDISC standards guides and Response Evaluation Criteria in Solid Tumors (RECIST) guidelines, the overall system architecture of CDISC-compliant CTIMS was designed. Modules for standard-compliant electronic case report form (eCRF) to verify data conformance  and transform into SDTM data format were developed by experts in diverse fields such as medical informatics, medical, and clinical trial. External validation of  the CDISC-compliant CTIMS was performed by comparing it with our previous CTIMS based on real-world data and CDISC validation rules by Pinnacle 21 Community Software. RESULTS: The architecture of CDISC-compliant CTIMS included the standard-compliant eCRF module of RECIST, the automatic verification module of the input data, and the SDTM transformation module from the eCRF input data to the SDTM datasets based on CDISC Define-XML. This new system was incorporated into our previous CTIMS. External validation demonstrated that all 176 human input errors occurred in the previous CTIMS filtered by a new system yielding zero error and CDISC-compliant dataset. The verified eCRF input data were automatically transformed into the SDTM dataset, which satisfied the CDISC validation rules by Pinnacle 21 Community Software. CONCLUSIONS: To assure data consistency and high quality of the tumor response assessment data, our new CTIMS can minimize human input error by using standard-compliant eCRF with an automatic verification module and automatically transform the datasets into CDISC SDTM format.;;Lee AJ, Kim KW, Shin Y, Lee J, Park HJ, Cho YC, Ko Y, Sung YS, Yoon BS;;eng;['Clinical Trials as Topic', 'Humans', '*Medical Informatics', '*Neoplasms/diagnostic imaging', 'Software'];['*CDISC', '*CTIMS', '*Clinical trial', '*RECIST', '*SDTM'];;;10.1016/j.jbi.2021.103782
38;33781919;Article;2021;Journal of biomedical informatics;;Development of a FHIR RDF data transformation and validation framework and its evaluation.;;Resource Description Framework (RDF) is one of the three standardized data formats in the HL7 Fast Healthcare Interoperability Resources (FHIR) specification and is being used by healthcare and research organizations to join  FHIR and non-FHIR data. However, RDF previously had not been integrated into popular FHIR tooling packages, hindering the adoption of FHIR RDF in the semantic web and other communities. The objective of the study is to develop and evaluate  a Java based FHIR RDF data transformation toolkit to facilitate the use and validation of FHIR RDF data. We extended the popular HAPI FHIR tooling to add RDF support, thus enabling FHIR data in XML or JSON to be transformed to or from RDF. We also developed an RDF Shape Expression (ShEx)-based validation framework to verify conformance of FHIR RDF data to the ShEx schemas provided in the FHIR specification for FHIR versions R4 and R5. The effectiveness of ShEx validation was demonstrated by testing it against 2693 FHIR R4 examples and 2197 FHIR R5 examples that are included in the FHIR specification. A total of 5 types of errors including missing properties, unknown element, missing resource Type, invalid attribute value, and unknown resource name in the R5 examples were revealed, demonstrating the value of the ShEx in the quality assurance of the evolving R5 development. This FHIR RDF data transformation and validation framework, based on HAPI and ShEx, is robust and ready for community use in adopting FHIR RDF, improving FHIR data quality, and evolving the FHIR specification.;;Prud'hommeaux E, Collins J, Booth D, Peterson KJ, Solbrig HR, Jiang G;;eng;['*Delivery of Health Care', '*Electronic Health Records'];['*Data transformation', '*Fast Healthcare Interoperability Resources (FHIR)', '*Quality assurance', '*Resource Description Framework (RDF)', '*Semantic web', '*Shape Expression (ShEx)'];;R01 EB030529/EB/NIBIB NIH HHS/United States;10.1016/j.jbi.2021.103755
39;33763309;Article;2021;PeerJ;;pmparser and PMDB: resources for large-scale, open studies of the biomedical literature.;;PubMed is an invaluable resource for the biomedical community. Although PubMed is freely available, the existing API is not designed for large-scale analyses and the XML structure of the underlying data is inconvenient for complex queries. We  developed an R package called pmparser to convert the data in PubMed to a relational database. Our implementation of the database, called PMDB, currently contains data on over 31 million PubMed Identifiers (PMIDs) and is updated regularly. Together, pmparser and PMDB can enable large-scale, reproducible, and  transparent analyses of the biomedical literature. pmparser is licensed under GPL-2 and available at https://pmparser.hugheylab.org. PMDB is available in both  PostgreSQL (DOI 10.5281/zenodo.4008109) and Google BigQuery (https://console.cloud.google.com/bigquery?project=pmdb-bq&d=pmdb).;;Schoenbachler JL, Hughey JJ;;eng;;['Database', 'Parsing', 'Publishing', 'Pubmed'];Jacob J. Hughey is an Academic Editor for PeerJ.;R35 GM124685/GM/NIGMS NIH HHS/United States;10.7717/peerj.11071
40;33591796;Article;2021;JCO clinical cancer informatics;;Structured Data Capture for Oncology.;;Lack of interoperability is one of the greatest challenges facing healthcare informatics. Recent interoperability efforts have focused primarily on data transmission and generally ignore data capture standardization. Structured Data Capture (SDC) is an open-source technical framework that enables the capture and  exchange of standardized and structured data in interoperable data entry forms (DEFs) at the point of care. Some of SDC's primary use cases concern complex oncology data such as anatomic pathology, biomarkers, and clinical oncology data  collection and reporting. Its interoperability goals are the preservation of semantic, contextual, and structural integrity of the captured data throughout the data's lifespan. SDC documents are written in eXtensible Markup Language (XML) and are therefore computer readable, yet technology agnostic-SDC can be implemented by any EHR vendor or registry. Any SDC-capable system can render an SDC XML file into a DEF, receive and parse an SDC transmission, and regenerate the original SDC form as a DEF or synoptic report with the response data intact.  SDC is therefore able to facilitate interoperable data capture and exchange for patient care, clinical trials, cancer surveillance and public health needs, clinical research, and computable care guidelines. The usability of SDC-captured  oncology data is enhanced when the SDC data elements are mapped to standard terminologies. For example, an SDC map to Systematized Nomenclature of Medicine Clinical Terms (SNOMED CT) enables aggregation of SDC data with other related data sets and permits advanced queries and groupings on the basis of SNOMED CT concept attributes and description logic. SDC supports terminology maps using separate map files or as terminology codes embedded in an SDC document.;;Goel AK, Campbell WS, Moldwin R;;eng;['Delivery of Health Care', 'Humans', 'Medical Oncology', '*Semantics', '*Systematized Nomenclature of Medicine'];;;;10.1200/CCI.20.00103
41;33490264;Article;2021;BioMed research international;;Data-Driven Discovery of Molecular Targets for Antibody-Drug Conjugates in Cancer Treatment.;;Antibody-drug conjugate therapy has attracted considerable attention in recent years. Since the selection of appropriate targets is a critical aspect of antibody-drug conjugate research and development, a big data research for discovery of candidate targets per tumor type is outstanding and of high interest. Thus, the purpose of this study was to identify and prioritize candidate antibody-drug conjugate targets with translational potential across common types of cancer by mining the Human Protein Atlas, as a unique big data resource. To perform a multifaceted screening process, XML and TSV files including immunohistochemistry expression data for 45 normal tissues and 20 tumor types were downloaded from the Human Protein Atlas website. For genes without high protein expression across critical normal tissues, a quasi H-score (range, 0-300) was computed per tumor type. All genes with a quasi H - score >/= 150 were extracted. Of these, genes with cell surface localization were selected and included in a multilevel validation process. Among 19670 genes that encode proteins, 5520 membrane protein-coding genes were included in this study. During  a multistep data mining procedure, 332 potential targets were identified based on the level of the protein expression across critical normal tissues and 20 tumor types. After validation, 23 cell surface proteins were identified and prioritized as candidate antibody-drug conjugate targets of which two have interestingly been approved by the FDA for use in solid tumors, one has been approved for lymphoma,  and four have currently been entered in clinical trials. In conclusion, we identified and prioritized several candidate targets with translational potential, which may yield new clinically effective and safe antibody-drug conjugates. This large-scale antibody-based proteomic study allows us to go beyond the RNA-seq studies, facilitates bench-to-clinic research of targeted anticancer therapeutics, and offers valuable insights into the development of new antibody-drug conjugates.;;Razzaghdoust A, Rahmatizadeh S, Mofid B, Muhammadnejad S, Parvin M, Torbati PM, Basiri A;;eng;['*Antineoplastic Agents/chemistry/metabolism', 'Data Mining', 'Databases, Genetic', 'Drug Discovery/*methods', 'Humans', '*Immunoconjugates/chemistry/metabolism', 'Membrane Proteins/genetics/metabolism', '*Neoplasms/drug therapy/genetics/metabolism', 'Proteomics/*methods'];;The authors declare that there is no conflict of interest regarding the publication of this paper.;;10.1155/2021/2670573
42;32864978;Article;2021;Journal of proteome research;;mzMLb: A Future-Proof Raw Mass Spectrometry Data Format Based on Standards-Compliant mzML and Optimized for Speed and Storage Requirements.;;"With ever-increasing amounts of data produced by mass spectrometry (MS) proteomics and metabolomics, and the sheer volume of samples now analyzed, the need for a common open format possessing both file size efficiency and faster read/write speeds has become paramount to drive the next generation of data analysis pipelines. The Proteomics Standards Initiative (PSI) has established a clear and precise extensible markup language (XML) representation for data interchange, mzML, receiving substantial uptake; nevertheless, storage and file access efficiency has not been the main focus. We propose an HDF5 file format ""mzMLb"" that is optimized for both read/write speed and storage of the raw mass spectrometry data. We provide an extensive validation of the write speed, random  read speed, and storage size, demonstrating a flexible format that with or without compression is faster than all existing approaches in virtually all cases, while with compression is comparable in size to proprietary vendor file formats. Since our approach uniquely preserves the XML encoding of the metadata,  the format implicitly supports future versions of mzML and is straightforward to  implement: mzMLb's design adheres to both HDF5 and NetCDF4 standard implementations, which allows it to be easily utilized by third parties due to their widespread programming language support. A reference implementation within  the established ProteoWizard toolkit is provided.";;Bhamber RS, Jankevics A, Deutsch EW, Jones AR, Dowsey AW;;eng;['Databases, Protein', 'Mass Spectrometry', 'Metabolomics', '*Programming Languages', '*Proteomics', 'Software'];['*HDF5', '*Proteomics Standards Initiative', '*data compression', '*mass spectrometry', '*metabolomics', '*mzML', '*proteomics'];;R01 GM087221/GM/NIGMS NIH HHS/United States;10.1021/acs.jproteome.0c00192
43;32415977;Article;2021;Systematic biology;;Markov-Modulated Continuous-Time Markov Chains to Identify Site- and Branch-Specific Evolutionary Variation in BEAST.;;"Markov models of character substitution on phylogenies form the foundation of phylogenetic inference frameworks. Early models made the simplifying assumption that the substitution process is homogeneous over time and across sites in the molecular sequence alignment. While standard practice adopts extensions that accommodate heterogeneity of substitution rates across sites, heterogeneity in the process over time in a site-specific manner remains frequently overlooked. This is problematic, as evolutionary processes that act at the molecular level are highly variable, subjecting different sites to different selective constraints over time, impacting their substitution behavior. We propose incorporating time variability through Markov-modulated models (MMMs), which extend covarion-like models and allow the substitution process (including relative character exchange rates as well as the overall substitution rate) at individual sites to vary across lineages. We implement a general MMM framework in BEAST, a popular Bayesian phylogenetic inference software package, allowing researchers to compose a wide range of MMMs through flexible XML specification. Using examples from bacterial, viral, and plastid genome evolution, we show that  MMMs impact phylogenetic tree estimation and can substantially improve model fit  compared to standard substitution models. Through simulations, we show that marginal likelihood estimation accurately identifies the generative model and does not systematically prefer the more parameter-rich MMMs. To mitigate the increased computational demands associated with MMMs, our implementation exploits recent developments in BEAGLE, a high-performance computational library for phylogenetic inference. [Bayesian inference; BEAGLE; BEAST; covarion, heterotachy; Markov-modulated models; phylogenetics.].";;Baele G, Gill MS, Bastide P, Lemey P, Suchard MA;;eng;['Bayes Theorem', 'Computer Simulation', '*Evolution, Molecular', 'Markov Chains', '*Models, Genetic', 'Phylogeny', 'Sequence Alignment'];;;U19 AI135995/AI/NIAID NIH HHS/United States;10.1093/sysbio/syaa037
44;35198447;Article;2022;Frontiers in oncology;;A Bibliometric Analysis of the Landscape of Parathyroid Carcinoma Research Based  on the PubMed (2000-2021).;;"Introduction: The purpose of this study was to assess the landscape of parathyroid carcinoma research during the last 22 years using machine learning and text analysis. Method: In November 2021, we obtained from PubMed all works indexed under the mesh subject line ""parathyroid carcinoma"". The entire set of search results was retrieved in XML format, and metadata such as title, abstract, keywords, mesh words, and year of publication were extracted for bibliometric evaluation from the original XML files. To increase the specificity of the investigation, the Latent Dirichlet allocation (LDA) topic modeling method was applied. Results: The paper analyzed 3578 papers. The volume of literature related to parathyroid cancer has been relatively flat over the past 22 years. In the field of parathyroid cancer research, the most important topic of clinical interest is the differential diagnosis. Ultrasound and MIBI are the most commonly used imaging methods for localization. In terms of basic research, the mechanisms of gene mutation and local tumor recurrence are the focus of interest. Conclusion: There are huge unmet research needs for parathyroid carcinoma. Improving the diagnosis rates of parathyroid cancer by clinicians and establishing new and reliable molecular pathological markers and new image localization techniques will continue to be the focus of future research.";;Feng C, Tian C, Huang L, Chen H, Feng Y, Chang S;;eng;;['PubMed', 'machine learning', 'natural language processing', 'parathyroid carcinoma', 'publication analysis'];The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.;;10.3389/fonc.2022.824201
45;35175199;Article;2022;JMIR human factors;;Usage and Usability of a National e-Library for Chemotherapy Regimens: Mixed Methods Study.;;BACKGROUND: Accurate information about chemotherapy drugs and regimens is needed  to reduce chemotherapy errors. A national e-library, as a common knowledge source with standardized chemotherapy nomenclature and content, was developed. Since the information in the library is both complex and extensive, it is central that the  users can use the resource as intended. OBJECTIVE: The aim of this study was to evaluate the usage and usability of an extensive e-library for chemotherapy regimens developed to reduce medication errors, support the health care staff in  their work, and increase patient safety. METHODS: To obtain a comprehensive evaluation, a mixed methods study was performed for a broad view of the usage, including a compilation of subjective views of the users (web survey, spontaneous user feedback, and qualitative interviews), analysis of statistics from the website, and an expert evaluation of the usability of the webpage. RESULTS: Statistics from the website show an average of just over 2500 visits and 870 unique visitors per month. Most visits took place Mondays to Fridays, but there were 5-10 visits per day on weekends. The web survey, with 292 answers, shows that the visitors were mainly physicians and nurses. Almost 80% (224/292) of respondents searched for regimens and 90% (264/292) found what they were looking  for and were satisfied with their visit. The expert evaluation shows that the e-library follows many existing design principles, thus providing some useful improvement suggestions. A total of 86 emails were received in 2020 with user feedback, most of which were from nurses. The main part (78%, 67/86) contained a  question, and the rest had discovered errors mainly in some regimen. The interviews reveal that most hospitals use a computerized physician order entry system, and they use the e-library in various ways, import XML files, transfer information, or use it as a reference. One hospital without a system uses the administration schedules from the library. CONCLUSIONS: The user evaluation indicates that the e-library is used in the intended manner and that the users can interact without problems. Users have different needs depending on their profession and their workplace, and these can be supported. The combination of methods applied ensures that the design and content comply with the users' needs  and serves as feedback for continuous design and learning. With a broad national  usage, the e-library can become a source for organizational and national learning and a source for continuous improvement of cancer care in Sweden.;;Fyhr A, Persson J, Ek A;;eng;;['chemotherapy', 'chemotherapy regimens', 'e-library', 'medication errors', 'patient safety', 'safety', 'standardization', 'usability', 'user evaluation'];;;10.2196/33651
46;35172094;Article;2022;Healthcare informatics research;;ANNO: A General Annotation Tool for Bilingual Clinical Note Information Extraction.;;Objective: This study was conducted to develop a generalizable annotation tool for bilingual complex clinical text annotation, which led to the design and development of a clinical text annotation tool, ANNO. Methods: We designed ANNO to enable human annotators to support the annotation of information in clinical documents efficiently and accurately. First, annotations for different classes (word or phrase types) can be tagged according to the type of word using the dictionary function. In addition, it is possible to evaluate and reconcile differences by comparing annotation results between human annotators. Moreover, if the regular expression set for each class is updated during annotation, it is  automatically reflected in the new document. The regular expression set created by human annotators is designed such that a word tagged once is automatically labeled in new documents. RESULTS: Because ANNO is a Docker-based web application, users can use it freely without being subjected to dependency issues. Human annotators can share their annotation markups as regular expression sets with a dictionary structure, and they can cross-check their annotated corpora with each other. The dictionary-based regular expression sharing function, cross-check function for each annotator, and standardized input (Microsoft Excel) and output (extensible markup language [XML]) formats are the main features of ANNO. Conclusions: With the growing need for massively annotated clinical data to support the development of machine learning models, we expect ANNO to be helpful to many researchers.;;Lee KH, Lee H, Park JH, Kim YJ, Lee Y;;eng;;['Data Mining', 'Information Storage and Retrieval', 'Medical Records', 'Personal Health Records'];;Korea Health Industry Development Institute;10.4258/hir.2022.28.1.89
47;35141226;Article;2022;Frontiers in cell and developmental biology;;Putative Prevention of XML Injection Against Myocardial Ischemia Is Mediated by PKC and PLA2 Proteins.;;Background: Xinmailong (XML) injection is a CFDA-approved traditional Chinese medicine with clinical value for heart failure treatment. The present investigation was aimed to evaluate the potential protective roles of this injection on myocardial ischemia and the underlying molecular mechanism. Methods: In our study, we selected two models of myocardial ischemia rats. Rats were randomly divided into six groups, with saline or XML administrated 4 days before  ischemia model establishment. ECG of different time intervals and biochemical parameters of end point were measured. The potential mechanisms of the protective role of XML were explored using system pharmacology and molecular biology approaches. Results: Myocardial ischemia rats demonstrated abnormal ECG and serum levels of cTnT. Pretreatment with XML significantly attenuated these damages, especially the medium doses. GO and KEGG analysis revealed that the 90 putative target genes were associated with pathways of fatty acid absorption/metabolism, inflammation, RAAS, and vascular smooth muscle. Further network pharmacology method identified five main chemical ingredients and potential targets of XML injection for myocardial ischemia. Mechanically, the beneficial effect of XML injection was mediated by the reactive oxygen species (ROS) inhibition and inflammation attenuation via regulating the expression levels of targets of PKC and PLA2. Conclusion: These findings indicate that XML exerts protective effects  against myocardial injury, with attenuated ROS production, apoptosis, and inflammation. Therefore, we speculate that XML may be an alternative supplementary therapeutic agent for myocardial ischemia prevention.;;Jin L, Yin Q, Mao Y, Gao Y, Han Q, Mei R, Xue L, Tan H, Li H;;eng;;['PLA 2', 'Protein kinase C', 'myocardial ischemia', 'network pharmacology', 'xinmailong injection'];The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.;;10.3389/fcell.2022.827691
48;35085089;Article;2022;IEEE/ACM transactions on computational biology and bioinformatics;;Next-generation sequencing markup language (NGSML): a medium for the representation and exchange of NGS data.;;With the increasing demand for low-cost high-throughput sequencing of large genomes, next-generation sequencing (NGS) technology has developed rapidly. NGS can not only be used in basic scientific research but also in clinical diagnostics and healthcare. Numerous software systems and tools have been developed to analyze NGS data, and various data formats have been produced to accommodate different sequencing equipment providers or analytical software. However, the data interoperability between these tools brings great challenges to researchers. A generic format that could be shared by most of the software and tools in the NGS field would make data interoperability and sharing easier. In this paper, we defined a general XML-based NGS markup language (NGSML) format for the representation and exchange of NGS data. We also developed a user-friendly GUI tool, NGSMLEditor, for presenting, creating, editing, and converting NGSML files. By using NGSML, various types of NGS data can be saved in one unified format. Compared with the unstructured plain text file, a structured data format  based on XML technology solves the incompatibility of various NGS data formats. The NGSML specifications are freely available from http://www.sysbio.org.cn/NGSML. NGSMLEditor is open source under GNU GPL and can  be downloaded from the website.;;Yu C, Qi X, Yan W, Wu W, Shen B;;eng;;;;;10.1109/TCBB.2022.3144170
49;35062172;Article;2022;Studies in health technology and informatics;;A Multilingual Browser Platform for Medical Subject Headings.;;The National Library of Medicine (NLM) controls and publishes the thesaurus Medical Subject Headings which is used for indexing PubMed. Besides an XML export, the NLM offers a web based MeSH browser. The platform contains English terms. The German Institute for Medical Documentation and Information (DIMDI) partially translated and published these terms. Recently, the German National Library of Medicine (ZB-MED) overtook the translation of MeSH. However, there is  no dedicated platform which focuses on MeSH and covers multiple languages. Here,  we address this gap, by offering a modern multilingual searchable MeSH browser. A modular platform using open source technology is presented. The frontend enables  the user to search and browse terms and switch between different languages. The current version of the presented MeSH browser contains English and German MeSH terms and can be accessed at https://mesh-browser.de.;;Scheible R, Strecker P, Yazijy S, Thomczyk F, Talpa R, Puhl A, Boeker M;;eng;['MEDLINE', '*Medical Subject Headings', 'National Library of Medicine (U.S.)', 'PubMed', 'United States', '*Vocabulary, Controlled'];['Databases', 'MeSH-Browser', 'Medical Subject Headings', 'Web Service'];;;10.3233/SHTI210939
50;34988566;Article;2022;Nanoscale;;A B2N monolayer: a direct band gap semiconductor with high and highly anisotropic carrier mobility.;;Two-dimensional materials with a planar lattice, suitable direct band gap, and high and highly anisotropic carrier mobility are desirable for the development of advanced field-effect transistors. Here we predict three thermodynamically stable B-rich 2D B-N compounds with the stoichiometries of B2N, B3N, and B4N using a combination of crystal structure searches and first-principles calculations. Among them, B2N has an ultraflat surface and consists of eight-membered B6N2 and  pentagonal B3N2 rings. The eight-membered B6N2 rings are linked to each other through both edge-sharing (in the y direction) and bridging B3N2 pentagons (in the x direction). B2N is a semiconductor with a direct band gap of 1.96 eV, and the nature of the direct band gap is well preserved in bilayer B2N. The hole mobility of B2N is as high as 0.6 x 10(3) cm(2) V(-1) s(-1) along the y direction, 7.5 times that in the x direction. These combined novel properties render the B2N monolayer as a natural example in the field of two-dimensional functional materials with broad application potential for use in field-effect transistors.;;Lin S, Guo Y, Xu M, Zhao J, Liang Y, Yuan X, Zhang Y, Wang F, Hao J, Li Y;;eng;;;;;10.1039/d1nr07054a
51;34614536;Article;2022;Rapid communications in mass spectrometry : RCM;;Open-source feature detection for non-target LC-MS analytics.;;RATIONALE: Non-target screening techniques using high-resolution mass spectrometers become more and more important for environmental sciences. Highly reliable and sophisticated software solutions are required to deal with the large amount of data obtained from such analyses. METHODS: Processing of high-resolution LC-HRMS data was performed upon conversion into an open, XML-based data format followed by an automated assignment of chromatographic peaks using the open-source programming language R. Raw data from three different LC-HRMS systems were processed as a proof of principle. RESULTS: We present a simple and straightforward algorithm to extract chromatographic peaks from previously m/z-centroided data based on the open-source programming language R and C++. The working principle and processing parameters are explained in detail. A ready-to-use script is provided in the supporting information. CONCLUSIONS: The developed algorithm enables a comprehensible automated peak picking of non-target LC-MS data. Application to three completely different HRMS raw data files showed  reasonable False Positives and False Negatives detection and moderate calculation times.;;Dietrich C, Wick A, Ternes TA;;eng;;;;02WRM1367C/Bundesministerium fur Bildung und Forschung;10.1002/rcm.9206
